## 종합 테스트 계획서

**문서 버전:** 1.0
**작성일:** 2025년 7월 21일
**작성자:** Gemini

### 1. 서론

본 문서는 감성 분석 앙상블 모델 프로젝트의 품질 보증을 위한 종합 테스트 계획을 정의합니다. 이 계획은 프로젝트 1(감성 분석 모델 구현 및 학습), 프로젝트 2(점진적 자기지도학습 기반 데이터셋 정제 및 모델 재훈련), 프로젝트 3(앙상블 모델 기반 감성 분석 API 서비스)의 모든 단계에 걸쳐 시스템의 기능, 성능, 신뢰성 및 재현성을 검증하는 것을 목표로 합니다.

### 2. 테스트 범위 (Test Scope)

#### 2.1. 포함 범위

*   **프로젝트 1:**
    *   원시 데이터 통합 및 SQLite DB 저장 기능 검증.
    *   KSS를 이용한 문장 분리 및 임시 라벨링 기능 검증.
    *   균형 잡힌 초기 데이터셋 생성 및 DVC 버전 관리 기능 검증.
    *   Word Sentiment, LSTM, BiLSTM, BERT 모델의 개별 학습 및 평가 기능 검증.
    *   MLflow를 통한 실험 추적 및 아티팩트 관리 기능 검증.
*   **프로젝트 2:**
    *   SQLite DB에서 청크 단위 데이터 로딩 및 앙상블 예측 기능 검증.
    *   신뢰도 기반 라벨 보정 및 DB 업데이트 기능 검증.
    *   Gemini API를 활용한 재라벨링, 정성적 평가, 편향 감지/완화 기능 검증.
    *   각 모델별 최적화된 재훈련 전략(파인튜닝, 부분 재훈련 등) 검증.
    *   자기지도학습 루프의 수렴 감지 및 확증 편향 완화 전략 검증.
    *   MLflow 및 DVC를 통한 재훈련 과정 및 데이터셋 버전 관리 기능 검증.
*   **프로젝트 3:**
    *   FastAPI 기반 `POST /predict` API 엔드포인트의 기능 및 유효성 검증.
    *   MLflow Model Registry에서 최신 모델 로딩 및 예측 기능 검증.
    *   모델 자동 교체(Hot-swapping) 메커니즘의 무중단 및 정확성 검증.
    *   Prometheus/Grafana를 통한 API 모니터링 메트릭 노출 및 시각화 검증.
    *   Docker 컨테이너화 및 배포 일관성 검증.

#### 2.2. 제외 범위

*   서드파티 라이브러리(Pandas, NumPy, KSS, KoNLPy, Scikit-learn, TensorFlow, PyTorch, Hugging Face Transformers, MLflow, DVC, FastAPI, Prometheus, Grafana, Docker) 자체의 내부 기능 테스트.
*   운영 환경의 인프라(클라우드, 서버 하드웨어 등) 테스트.
*   보안 취약점 분석(초기 단계에서는 제외).

### 3. 테스트 목표 (Test Objectives)

*   모든 기능 요구사항이 명세에 따라 올바르게 구현되었는지 확인합니다.
*   시스템이 정의된 성능 요구사항(API 응답 시간, RPS 등)을 충족하는지 검증합니다.
*   데이터 처리 및 모델 학습/재훈련 과정의 신뢰성과 안정성을 보장합니다.
*   MLOps 파이프라인(MLflow, DVC)을 통한 실험 재현성 및 관리 체계의 유효성을 검증합니다.
*   모델 자동 교체 메커니즘이 서비스 중단 없이 작동하는지 확인합니다.
*   데이터셋 및 모델의 잠재적 편향이 적절히 감지되고 완화되는지 평가합니다.

### 4. 테스트 유형 (Test Types)

*   **단위 테스트 (Unit Testing):** 각 함수, 클래스, 모듈 등 최소 단위의 코드에 대한 기능 검증.
*   **통합 테스트 (Integration Testing):** 여러 모듈 또는 시스템 구성 요소 간의 상호 작용 및 데이터 흐름 검증.
*   **시스템 테스트 (System Testing):** 전체 시스템이 기능 요구사항 및 비기능 요구사항을 충족하는지 종단 간(End-to-End) 검증.
*   **성능 테스트 (Performance Testing):** 시스템의 응답 시간, 처리량, 자원 사용량 등을 측정하여 성능 요구사항 충족 여부 검증 (부하 테스트, 스트레스 테스트 포함).
*   **회귀 테스트 (Regression Testing):** 코드 변경 또는 업데이트 후 기존 기능이 올바르게 작동하는지 확인.
*   **재현성 테스트 (Reproducibility Testing):** DVC 태그 및 MLflow 로그를 사용하여 특정 실험 결과(데이터셋, 모델, 성능)가 재현 가능한지 검증.
*   **사용성 테스트 (Usability Testing):** (API 사용성 관점에서) API 호출의 용이성 및 응답의 명확성 검증.

### 5. 테스트 환경 (Test Environment)

*   **개발 환경:** 각 개발자의 로컬 머신 (Python 가상 환경, Docker Desktop).
*   **테스트 환경:** 개발 환경과 유사한 구성의 전용 테스트 서버 또는 클라우드 인스턴스.
    *   **OS:** Ubuntu 20.04+ (또는 호환 가능한 Linux 배포판).
    *   **CPU:** 4코어 8스레드 이상.
    *   **RAM:** 16GB 이상.
    *   **GPU:** NVIDIA GPU (GTX 960, 1070, 1080Ti 등) 및 CUDA/cuDNN 설치.
    *   **소프트웨어:** Python 3.9+, Conda, Docker, MLflow Server, Prometheus, Grafana.

### 6. 테스트 데이터 관리 (Test Data Management)

*   **원본 데이터:** `y_review.csv`, `naver_shopping.txt`와 같은 원본 리뷰 데이터셋을 테스트 데이터로 활용합니다.
*   **생성 데이터:** KSS를 통해 생성된 문장 데이터, 초기 학습 데이터셋(`v1.0-initial-20k.csv`), 자기지도학습을 통해 정제된 데이터셋 등을 테스트 데이터로 활용합니다.
*   **데이터 버전 관리:** DVC를 사용하여 모든 테스트 데이터셋의 버전을 관리하여 테스트 재현성을 보장합니다.
*   **민감 데이터 처리:** 실제 리뷰 데이터에 민감 정보가 포함될 경우, 테스트 환경에서는 비식별화 또는 가명 처리된 데이터를 사용합니다.

### 7. 테스트 도구 (Test Tools)

*   **단위/통합 테스트:** `pytest`, `unittest` (Python).
*   **API 테스트:** `requests` (Python), `Postman`, `curl`.
*   **성능 테스트:** `JMeter`, `Locust`.
*   **코드 커버리지:** `pytest-cov`.
*   **MLOps:** `MLflow`, `DVC`.
*   **모니터링:** `Prometheus`, `Grafana`.

### 8. 테스트 일정 (Test Schedule)

각 프로젝트 단계별로 테스트 활동을 통합하여 진행합니다.

*   **프로젝트 1:**
    *   단위 테스트: 각 모듈 개발 완료 시점.
    *   통합 테스트: 데이터 파이프라인 및 모델 학습 파이프라인 구축 완료 시점.
    *   시스템 테스트: 베이스라인 모델 학습 완료 후.
*   **프로젝트 2:**
    *   단위 테스트: 각 모듈(데이터 로딩, 예측, 라벨 보정, 재훈련) 개발 완료 시점.
    *   통합 테스트: 자기지도학습 루프 구축 완료 시점.
    *   성능 테스트: 재훈련 파이프라인의 효율성 검증.
*   **프로젝트 3:**
    *   단위 테스트: API 엔드포인트, 모델 로딩, 핫스왑 로직 개발 완료 시점.
    *   통합 테스트: API 서비스와 MLflow, Prometheus/Grafana 연동 완료 시점.
    *   성능 테스트: API 응답 시간 및 RPS 검증.
    *   회귀 테스트: 모델 업데이트 후 기존 기능 검증.

### 9. 역할 및 책임 (Roles and Responsibilities)

*   **프로젝트 관리자:** 테스트 계획 승인, 자원 할당, 주요 의사 결정.
*   **개발자:** 단위 테스트 작성 및 수행, 통합 테스트 지원, 결함 수정.
*   **ML 엔지니어:** 모델 관련 테스트(성능, 재현성, 편향) 설계 및 수행, MLflow/DVC 관리.
*   **QA 엔지니어 (또는 전담 테스터):** 통합 테스트, 시스템 테스트, 성능 테스트, 회귀 테스트 계획 및 수행, 결함 보고 및 추적.

### 10. 결함 관리 (Defect Management)

*   **결함 보고:** 발견된 모든 결함은 상세한 정보(재현 단계, 예상 결과, 실제 결과, 스크린샷/로그 등)와 함께 보고 시스템(예: Jira, GitHub Issues)에 등록합니다.
*   **결함 우선순위:** 결함의 심각도(Critical, Major, Minor, Trivial) 및 우선순위(High, Medium, Low)를 정의하고 할당합니다.
*   **결함 추적:** 보고된 결함은 수정될 때까지 추적하며, 수정 후에는 재테스트를 통해 결함이 해결되었는지 확인합니다.

### 11. 테스트 산출물 (Test Deliverables)

*   테스트 계획서 (본 문서).
*   테스트 케이스 문서.
*   테스트 실행 보고서 (각 테스트 유형별).
*   결함 보고서.
*   성능 테스트 보고서.
*   최종 테스트 요약 보고서.

### 12. 종료 기준 (Exit Criteria)

*   모든 Critical 및 Major 결함이 해결되고 재테스트를 통과했습니다.
*   모든 테스트 유형(단위, 통합, 시스템, 성능, 회귀, 재현성)이 계획에 따라 완료되었습니다.
*   코드 커버리지 목표(예: 80% 이상)가 달성되었습니다.
*   성능 요구사항이 충족되었습니다.
*   모든 이해관계자가 시스템의 품질에 동의했습니다.
