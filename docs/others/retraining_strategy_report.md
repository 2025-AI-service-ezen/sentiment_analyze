# ğŸ” 4ëª¨ë¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ ì¬í›ˆë ¨ ë°©ë²•ë¡  ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ

**ì‘ì„±ì¼**: 2025ë…„ 7ì›” 17ì¼  
**í”„ë¡œì íŠ¸**: ê°ì„±ë¶„ì„ 4ëª¨ë¸ ì•™ìƒë¸” ì‹œìŠ¤í…œ  
**ë¶„ì„ ë²”ìœ„**: LSTM, BiLSTM, BERT, Word Sentiment ëª¨ë¸

---

## ğŸ“‹ Executive Summary

ì´ í”„ë¡œì íŠ¸ì˜ ì¬í›ˆë ¨ì€ **ì™„ì „ ì¬í›ˆë ¨ì´ ì•„ë‹Œ ì „ëµì  ì°¨ë³„í™” ì ‘ê·¼ë²•**ì„ ì·¨í•´ì•¼ í•©ë‹ˆë‹¤. ê° ëª¨ë¸ë³„ë¡œ ìµœì ì˜ ì¬í›ˆë ¨ ì „ëµì´ ë‹¤ë¥´ë©°, ë°ì´í„° ì •ì œ ì‹œìŠ¤í…œê³¼ ì—°ê³„ëœ ì ì§„ì  ê°œì„  ë°©ì‹ì´ ê°€ì¥ íš¨ìœ¨ì ì…ë‹ˆë‹¤.

### ğŸ¯ í•µì‹¬ ê²°ë¡ 
- **ì¬í›ˆë ¨ ì •ì˜**: ì •ì œëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ë³„ ìµœì í™”ëœ ë°©ë²•ë¡  ì ìš©
- **ì™„ì „ ì¬í›ˆë ¨ vs ë¶€ë¶„ ì¬í›ˆë ¨**: ëª¨ë¸ íŠ¹ì„±ì— ë”°ë¥¸ ì°¨ë³„í™” ì „ëµ
- **ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**: ì•™ìƒë¸” ì •í™•ë„ 90-95% ë‹¬ì„± ëª©í‘œ
- **ë¹„ìš© íš¨ìœ¨ì„±**: ì™„ì „ ì¬í›ˆë ¨ ëŒ€ë¹„ 70% ì ì€ ì»´í“¨íŒ… ë¹„ìš©

---

## ğŸ¯ ê° ëª¨ë¸ë³„ ìµœì  ì¬í›ˆë ¨ ì „ëµ

### 1. **BERT ëª¨ë¸ (30% ê°€ì¤‘ì¹˜)**
**ğŸ”„ ê¶Œì¥: íŒŒì¸íŠœë‹ (Fine-tuning)**

#### í˜„ì¬ êµ¬í˜„ ë¶„ì„
- **íŒŒì¼**: `src/training/train_lstm.py` (ì‹¤ì œë¡œëŠ” BERT í›ˆë ¨)
- **ë°©ë²•**: `TFAutoModelForSequenceClassification.from_pretrained()` 
- **íŠ¹ì§•**: ì‚¬ì „í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ í™œìš©, ë¶„ë¥˜ í—¤ë“œ ìƒˆë¡œ ì´ˆê¸°í™”

#### ì¬í›ˆë ¨ ì „ëµ
```python
# âœ… ê¶Œì¥ êµ¬í˜„ ë°©ë²•
def retrain_bert_finetuning(refined_data):
    """BERT íŒŒì¸íŠœë‹ ì¬í›ˆë ¨"""
    # 1. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ë¶„ë¥˜ í—¤ë“œ ì œì™¸)
    model = TFAutoModelForSequenceClassification.from_pretrained(
        BERT_MODEL_NAME, 
        num_labels=5
    )
    
    # 2. ë¶„ë¥˜ í—¤ë“œë§Œ ì¬ì´ˆê¸°í™”
    model.classifier.layers[-1].kernel.initializer = 'glorot_uniform'
    
    # 3. í•™ìŠµë¥  ì°¨ë³„í™”
    optimizer = tf.keras.optimizers.Adam([
        {'params': model.bert.parameters(), 'lr': 1e-5},  # encoder
        {'params': model.classifier.parameters(), 'lr': 1e-3}  # classifier
    ])
    
    # 4. ì ì§„ì  í•´ë™ (Progressive unfreezing)
    for epoch in range(epochs):
        if epoch > 2:  # 3ë²ˆì§¸ ì—í­ë¶€í„° ìƒìœ„ ë ˆì´ì–´ í•´ë™
            unfreeze_top_layers(model, num_layers=2)
```

#### ì¬í›ˆë ¨ ì´ìœ 
- âœ… **ì‚¬ì „í›ˆë ¨ëœ ì–¸ì–´ í‘œí˜„ í™œìš©**: ì™„ì „ ì¬í›ˆë ¨ë³´ë‹¤ íš¨ìœ¨ì 
- âœ… **ë„ë©”ì¸ ì ì‘**: ê°ì„±ë¶„ì„ íƒœìŠ¤í¬ì— íŠ¹í™”ëœ ë¯¸ì„¸ì¡°ì •
- âœ… **ì•ˆì •ì  ìˆ˜ë ´**: ì¢‹ì€ ì´ˆê¸°ê°’ì—ì„œ ì‹œì‘í•˜ì—¬ ë¹ ë¥¸ ìˆ˜ë ´

---

### 2. **BiLSTM ëª¨ë¸ (30% ê°€ì¤‘ì¹˜)**
**ğŸ”„ ê¶Œì¥: ë¶€ë¶„ ì¬í›ˆë ¨ (Partial Retraining)**

#### í˜„ì¬ êµ¬í˜„ ë¶„ì„
- **íŒŒì¼**: `src/training/train_bilstm.py`
- **ë°©ë²•**: PyTorch ê¸°ë°˜ ì²˜ìŒë¶€í„° í›ˆë ¨
- **íŠ¹ì§•**: ì–‘ë°©í–¥ LSTM + ì„ë² ë”© ë ˆì´ì–´

#### ì¬í›ˆë ¨ ì „ëµ
```python
# âœ… ê¶Œì¥ êµ¬í˜„ ë°©ë²•
def retrain_bilstm_partial(refined_data):
    """BiLSTM ë¶€ë¶„ ì¬í›ˆë ¨"""
    # 1. ê¸°ì¡´ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load('models/bilstm/best_model.pt')
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # 2. ì„ë² ë”© ë ˆì´ì–´ ê³ ì • (ê¸°ì¡´ ë‹¨ì–´ í‘œí˜„ ìœ ì§€)
    for param in model.embedding.parameters():
        param.requires_grad = False
    
    # 3. BiLSTMê³¼ FC ë ˆì´ì–´ë§Œ ì¬í›ˆë ¨
    trainable_params = [p for p in model.parameters() if p.requires_grad]
    optimizer = optim.Adam(trainable_params, lr=1e-3)
    
    # 4. ì€ë‹‰ ìƒíƒœ ì´ˆê¸°í™” ì „ëµ
    def init_hidden_states():
        return torch.zeros(num_layers*2, batch_size, hidden_size)
```

#### ì¬í›ˆë ¨ ì´ìœ 
- âœ… **ì„ë² ë”© ì•ˆì •ì„±**: ê¸°ì¡´ í•™ìŠµëœ ë‹¨ì–´ ì„ë² ë”© ë³´ì¡´
- âœ… **íŒ¨í„´ í•™ìŠµ ì§‘ì¤‘**: LSTM ë ˆì´ì–´ê°€ ìƒˆë¡œìš´ ì‹œí€€ìŠ¤ íŒ¨í„´ í•™ìŠµ
- âœ… **íš¨ìœ¨ì  ìˆ˜ë ´**: ì „ì²´ ì¬í›ˆë ¨ ëŒ€ë¹„ ë¹ ë¥¸ ìˆ˜ë ´

---

### 3. **LSTM ëª¨ë¸ (25% ê°€ì¤‘ì¹˜)**
**ğŸ”„ ê¶Œì¥: ì „ì´í•™ìŠµ (Transfer Learning)**

#### í˜„ì¬ êµ¬í˜„ ë¶„ì„
- **ë°©ë²•**: ì™„ì „ ì¬í›ˆë ¨ (from scratch)
- **íŠ¹ì§•**: ë‹¨ë°©í–¥ LSTM ì•„í‚¤í…ì²˜

#### ì¬í›ˆë ¨ ì „ëµ
```python
# âœ… ê¶Œì¥ êµ¬í˜„ ë°©ë²•
def retrain_lstm_transfer(refined_data):
    """LSTM ì „ì´í•™ìŠµ ì¬í›ˆë ¨"""
    # 1. ëª¨ë¸ ì•„í‚¤í…ì²˜ ìœ ì§€
    model = build_lstm_model(
        vocab_size=VOCAB_SIZE,
        embedding_dim=EMBEDDING_DIM,
        lstm_units=LSTM_HIDDEN_SIZE
    )
    
    # 2. ê°œì„ ëœ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
    for layer in model.layers:
        if hasattr(layer, 'kernel_initializer'):
            layer.kernel_initializer = 'he_normal'
    
    # 3. ì ì‘ì  í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-2,
        decay_steps=100,
        decay_rate=0.96
    )
    
    # 4. ì •ê·œí™” ê°•í™”
    model.add(tf.keras.layers.Dropout(0.3))
```

#### ì¬í›ˆë ¨ ì´ìœ 
- âœ… **êµ¬ì¡° ë‹¨ìˆœì„±**: ë‹¨ë°©í–¥ LSTMì€ ì™„ì „ ì¬í›ˆë ¨ì´ íš¨ê³¼ì 
- âœ… **ë¹ ë¥¸ í›ˆë ¨**: ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¥¸ í›ˆë ¨ ì‹œê°„
- âœ… **ê¸°ì¤€ì„  ì—­í• **: ë‹¤ë¥¸ ë³µì¡í•œ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ ê¸°ì¤€

---

### 4. **Word Sentiment ëª¨ë¸ (15% ê°€ì¤‘ì¹˜)**
**ğŸ”„ ê¶Œì¥: ì ì§„ì  ì‚¬ì „ ì—…ë°ì´íŠ¸ (Incremental Dictionary Update)**

#### í˜„ì¬ êµ¬í˜„ ë¶„ì„
- **íŒŒì¼**: `src/utils/data_refinement_system.py`ì—ì„œ ê´€ë¦¬
- **ë°©ë²•**: ì •ì  ê°ì„±ì‚¬ì „ ê¸°ë°˜ ë£° ì‹œìŠ¤í…œ
- **ì €ì¥ ìœ„ì¹˜**: `models/word_sentiment/`

#### ì¬í›ˆë ¨ ì „ëµ
```python
# âœ… ê¶Œì¥ êµ¬í˜„ ë°©ë²•
def update_sentiment_dictionary(refinement_results):
    """ì•™ìƒë¸” ê²°ê³¼ ê¸°ë°˜ ê°ì„±ì‚¬ì „ ì—…ë°ì´íŠ¸"""
    
    # 1. ê¸°ì¡´ ì‚¬ì „ ë¡œë“œ
    with open('models/word_sentiment/word_sentiment_dict.pickle', 'rb') as f:
        sentiment_dict = pickle.load(f)
    
    # 2. ìƒˆë¡œìš´ ë„ë©”ì¸ ë‹¨ì–´ ì¶”ê°€
    for word, sentiment_score in new_domain_words.items():
        if word not in sentiment_dict:
            sentiment_dict[word] = sentiment_score
    
    # 3. ê¸°ì¡´ ë‹¨ì–´ ì ìˆ˜ ë¯¸ì„¸ì¡°ì • (ì•™ìƒë¸” í”¼ë“œë°± í™œìš©)
    for word, ensemble_prediction in refinement_results.items():
        if word in sentiment_dict:
            old_score = sentiment_dict[word]
            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì ìˆ˜ ì—…ë°ì´íŠ¸
            sentiment_dict[word] = 0.7 * old_score + 0.3 * ensemble_prediction
    
    # 4. ë°˜ë³µë³„ ë°±ì—… ì €ì¥
    backup_path = f'models/word_sentiment/word_sentiment_dict_iter_{iteration}.pickle'
    with open(backup_path, 'wb') as f:
        pickle.dump(sentiment_dict, f)
```

#### ì¬í›ˆë ¨ ì´ìœ 
- âœ… **ë£° ê¸°ë°˜ íŠ¹ì„±**: ì „í†µì  ì˜ë¯¸ì˜ ì¬í›ˆë ¨ì´ ì•„ë‹Œ ì§€ì‹ ì—…ë°ì´íŠ¸
- âœ… **í•´ì„ ê°€ëŠ¥ì„±**: ë‹¨ì–´ë³„ ê°ì„± ì ìˆ˜ ì§ê´€ì  ì´í•´
- âœ… **ë¹ ë¥¸ ì ìš©**: ì‹¤ì‹œê°„ ì‚¬ì „ ì—…ë°ì´íŠ¸ ê°€ëŠ¥

---

## ğŸ”„ ë°ì´í„° ì •ì œ ì‹œìŠ¤í…œ ì—°ê³„ ì¬í›ˆë ¨ ì „ëµ

### ë°˜ë³µì  ì •ì œ-ì¬í›ˆë ¨ íŒŒì´í”„ë¼ì¸

```python
class IterativeRetrainingSystem:
    """
    ğŸ”„ ë°˜ë³µì  ì •ì œ-ì¬í›ˆë ¨ ì‹œìŠ¤í…œ
    =============================
    
    ë°ì´í„° ì •ì œì™€ ëª¨ë¸ ì¬í›ˆë ¨ì„ ë°˜ë³µí•˜ì—¬ 
    ì ì§„ì ìœ¼ë¡œ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” íŒŒì´í”„ë¼ì¸
    """
    
    def __init__(self, max_iterations=5, convergence_threshold=0.005):
        self.max_iterations = max_iterations
        self.convergence_threshold = convergence_threshold
        self.performance_history = []
    
    def refine_and_retrain_cycle(self):
        """ë©”ì¸ ë°˜ë³µ ì‚¬ì´í´"""
        
        for iteration in range(1, self.max_iterations + 1):
            print(f"\nğŸ”„ ë°˜ë³µ {iteration}/{self.max_iterations} ì‹œì‘")
            
            # 1ï¸âƒ£ í˜„ì¬ ì•™ìƒë¸”ë¡œ ë°ì´í„° ì •ì œ
            print("ğŸ“Š ë°ì´í„° ì •ì œ ì¤‘...")
            refined_data = self.ensemble.refine_dataset(
                self.training_data,
                confidence_threshold=0.8
            )
            
            # 2ï¸âƒ£ ê° ëª¨ë¸ë³„ ì°¨ë³„í™” ì¬í›ˆë ¨ (ìˆœì„œ ì¤‘ìš”)
            print("ğŸ¤– ëª¨ë¸ë³„ ì¬í›ˆë ¨ ì‹œì‘...")
            
            # Step 1: Word Sentiment (ê°€ì¥ ë¹ ë¦„)
            self.update_word_sentiment_dict(refined_data)
            
            # Step 2: LSTM (ê¸°ë³¸ íŒ¨í„´ í•™ìŠµ)
            self.retrain_lstm_transfer(refined_data)
            
            # Step 3: BiLSTM (ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸)
            self.retrain_bilstm_partial(refined_data)
            
            # Step 4: BERT (ê³ ë„í™”ëœ ì–¸ì–´ ì´í•´)
            self.retrain_bert_finetuning(refined_data)
            
            # 3ï¸âƒ£ ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
            performance = self.evaluate_ensemble()
            self.performance_history.append(performance)
            
            # 4ï¸âƒ£ ìˆ˜ë ´ ì¡°ê±´ í™•ì¸
            if self.check_convergence(performance):
                print(f"âœ… ìˆ˜ë ´ ì¡°ê±´ ë‹¬ì„± (ë°˜ë³µ {iteration})")
                break
            
            # 5ï¸âƒ£ ì¤‘ê°„ ê²°ê³¼ ì €ì¥
            self.save_iteration_checkpoint(iteration)
        
        return self.performance_history
    
    def check_convergence(self, current_performance):
        """ìˆ˜ë ´ ì¡°ê±´ í™•ì¸"""
        if len(self.performance_history) < 2:
            return False
        
        improvement = current_performance - self.performance_history[-2]
        return improvement < self.convergence_threshold
```

### ì¬í›ˆë ¨ ìˆœì„œ ìµœì í™” ì „ëµ

1. **Word Sentiment** (1ìˆœìœ„) 
   - â±ï¸ **ì†Œìš”ì‹œê°„**: ~5ë¶„
   - ğŸ¯ **ëª©ì **: ë¹ ë¥¸ ê¸°ì¤€ì„  ê°œì„ 

2. **LSTM** (2ìˆœìœ„)
   - â±ï¸ **ì†Œìš”ì‹œê°„**: ~30ë¶„
   - ğŸ¯ **ëª©ì **: ê¸°ë³¸ ì‹œí€€ìŠ¤ íŒ¨í„´ í•™ìŠµ

3. **BiLSTM** (3ìˆœìœ„) 
   - â±ï¸ **ì†Œìš”ì‹œê°„**: ~45ë¶„
   - ğŸ¯ **ëª©ì **: ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸ í™œìš©

4. **BERT** (4ìˆœìœ„)
   - â±ï¸ **ì†Œìš”ì‹œê°„**: ~2ì‹œê°„
   - ğŸ¯ **ëª©ì **: ê³ ìˆ˜ì¤€ ì–¸ì–´ ì´í•´ ìµœì í™”

---

## ğŸ“Š ì¬í›ˆë ¨ íš¨ê³¼ ì˜ˆì¸¡ ë° ì„±ëŠ¥ ì§€í‘œ

### ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒì¹˜

| ëª¨ë¸ | í˜„ì¬ ì •í™•ë„ | ì¬í›ˆë ¨ í›„ ì˜ˆìƒ | ê°œì„ í­ | ì¬í›ˆë ¨ ë°©ë²• |
|------|-------------|----------------|--------|------------|
| **BERT** | 96%+ | 97-98% | +1-2% | íŒŒì¸íŠœë‹ |
| **BiLSTM** | ì§„í–‰ì¤‘ | 85-90% | ê¸°ì¤€ì„  ì„¤ì • | ë¶€ë¶„ ì¬í›ˆë ¨ |
| **LSTM** | ì™„ë£Œ | 82-87% | +3-5% | ì „ì´í•™ìŠµ |
| **Word Sentiment** | 82% | 84-86% | +2-4% | ì‚¬ì „ ì—…ë°ì´íŠ¸ |
| **ğŸ¯ ì•™ìƒë¸”** | **ë¯¸ì¸¡ì •** | **90-95%** | **ìµœì í™” ëª©í‘œ** | **í†µí•© ì „ëµ** |

### ì¬í›ˆë ¨ ë¹„ìš© vs íš¨ê³¼ ë¶„ì„

```
ğŸ“ˆ íš¨ê³¼ ëŒ€ë¹„ ë¹„ìš© íš¨ìœ¨ì„±:

ğŸ¥‡ ë†’ì€ íš¨ê³¼ / ë‚®ì€ ë¹„ìš©:
   - Word Sentiment ì‚¬ì „ ì—…ë°ì´íŠ¸
   - ì˜ˆìƒ ê°œì„ : +2-4%, ë¹„ìš©: 5ë¶„

ğŸ¥ˆ ì¤‘ê°„ íš¨ê³¼ / ì¤‘ê°„ ë¹„ìš©:
   - BERT íŒŒì¸íŠœë‹: +1-2%, ë¹„ìš©: 2ì‹œê°„
   - BiLSTM ë¶€ë¶„ ì¬í›ˆë ¨: +ê¸°ì¤€ì„ , ë¹„ìš©: 45ë¶„

ğŸ¥‰ ë‚®ì€ íš¨ê³¼ / ë†’ì€ ë¹„ìš©:
   - ëª¨ë“  ëª¨ë¸ ì™„ì „ ì¬í›ˆë ¨
   - ì˜ˆìƒ ê°œì„ : +1-3%, ë¹„ìš©: 8ì‹œê°„+
```

### ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”

```python
# ì¬í›ˆë ¨ í›„ ê°€ì¤‘ì¹˜ ì¬ì¡°ì • ì „ëµ
optimal_weights = {
    'lstm': 0.20,      # 25% â†’ 20% (ê¸°ë³¸ íŒ¨í„´)
    'bilstm': 0.35,    # 30% â†’ 35% (í–¥ìƒëœ ì–‘ë°©í–¥)  
    'bert': 0.35,      # 30% â†’ 35% (ìµœì í™”ëœ íŒŒì¸íŠœë‹)
    'word_sentiment': 0.10  # 15% â†’ 10% (ë³´ì¡° ì—­í• )
}
```

---

## ğŸ›  êµ¬ì²´ì  êµ¬í˜„ ê¶Œì¥ì‚¬í•­

### 1. í†µí•© ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ

```python
class UnifiedCheckpointManager:
    """í†µí•© ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬ì"""
    
    def save_iteration_checkpoint(self, iteration):
        """ë°˜ë³µë³„ ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ì €ì¥"""
        checkpoint = {
            'iteration': iteration,
            'timestamp': datetime.now().isoformat(),
            
            # ëª¨ë¸ë³„ ìƒíƒœ
            'bert': {
                'model_state_dict': self.bert_model.state_dict(),
                'optimizer_state_dict': self.bert_optimizer.state_dict(),
                'accuracy': self.bert_accuracy,
                'loss': self.bert_loss
            },
            'bilstm': {
                'model_state_dict': self.bilstm_model.state_dict(),
                'optimizer_state_dict': self.bilstm_optimizer.state_dict(),
                'accuracy': self.bilstm_accuracy,
                'loss': self.bilstm_loss
            },
            'lstm': {
                'model_weights': self.lstm_model.get_weights(),
                'accuracy': self.lstm_accuracy,
                'loss': self.lstm_loss
            },
            'word_sentiment': {
                'dictionary_path': f'word_sentiment_dict_iter_{iteration}.pickle',
                'accuracy': self.word_sentiment_accuracy
            },
            
            # ì•™ìƒë¸” ìƒíƒœ
            'ensemble': {
                'weights': self.ensemble_weights,
                'accuracy': self.ensemble_accuracy,
                'precision': self.ensemble_precision,
                'recall': self.ensemble_recall,
                'f1_score': self.ensemble_f1
            },
            
            # ë°ì´í„° ì •ì œ ìƒíƒœ
            'data_refinement': {
                'refined_samples': self.refined_samples_count,
                'quality_score': self.data_quality_score,
                'refinement_rate': self.refinement_rate
            }
        }
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = f'checkpoints/iteration_{iteration}.pt'
        torch.save(checkpoint, checkpoint_path)
        
        # MLflow ë¡œê¹…
        self.log_to_mlflow(checkpoint)
```

### 2. ì¬í›ˆë ¨ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
def setup_retraining_monitoring():
    """MLflow ê¸°ë°˜ ì¬í›ˆë ¨ ì¶”ì  ì‹œìŠ¤í…œ"""
    
    # ì‹¤í—˜ ì„¤ì •
    mlflow.set_experiment("ensemble_retraining")
    
    with mlflow.start_run(run_name=f"retraining_{datetime.now().strftime('%Y%m%d_%H%M')}"):
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
        mlflow.log_param("max_iterations", 5)
        mlflow.log_param("convergence_threshold", 0.005)
        mlflow.log_param("confidence_threshold", 0.8)
        
        # ê° ë°˜ë³µë³„ ë©”íŠ¸ë¦­ ë¡œê¹…
        for iteration in range(1, 6):
            mlflow.log_metric("ensemble_accuracy", ensemble_acc, step=iteration)
            mlflow.log_metric("data_quality_score", quality_score, step=iteration)
            mlflow.log_metric("refinement_rate", refinement_rate, step=iteration)
            
            # ëª¨ë¸ë³„ ì„±ëŠ¥
            mlflow.log_metric("bert_accuracy", bert_acc, step=iteration)
            mlflow.log_metric("bilstm_accuracy", bilstm_acc, step=iteration)
            mlflow.log_metric("lstm_accuracy", lstm_acc, step=iteration)
            mlflow.log_metric("word_sentiment_accuracy", ws_acc, step=iteration)
```

### 3. ì¡°ê¸° ì¢…ë£Œ ë° ìˆ˜ë ´ ê°ì§€

```python
class ConvergenceDetector:
    """ìˆ˜ë ´ ê°ì§€ ë° ì¡°ê¸° ì¢…ë£Œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, patience=3, min_delta=0.005):
        self.patience = patience
        self.min_delta = min_delta
        self.wait = 0
        self.best_score = -np.inf
        
    def check_convergence(self, current_score):
        """ìˆ˜ë ´ ì¡°ê±´ í™•ì¸"""
        
        # 1. ì„±ëŠ¥ ê°œì„  í™•ì¸
        if current_score > self.best_score + self.min_delta:
            self.best_score = current_score
            self.wait = 0
            return False
        
        # 2. ëŒ€ê¸° ì¹´ìš´í„° ì¦ê°€
        self.wait += 1
        
        # 3. ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
        if self.wait >= self.patience:
            print(f"â¹ï¸  ì¡°ê¸° ì¢…ë£Œ: {self.patience}íšŒ ì—°ì† ê°œì„  ì—†ìŒ")
            return True
        
        return False
    
    def additional_convergence_checks(self):
        """ì¶”ê°€ ìˆ˜ë ´ ì¡°ê±´ë“¤"""
        
        # ë°ì´í„° ì •ì œìœ¨ í™•ì¸
        if self.refinement_rate < 0.01:  # 1% ë¯¸ë§Œ
            print("ğŸ“Š ë°ì´í„° ì •ì œìœ¨ ìˆ˜ë ´ (< 1%)")
            return True
        
        # ëª¨ë¸ ê°„ ì˜ˆì¸¡ ë¶„ì‚° í™•ì¸
        if self.prediction_variance < 0.05:  # 5% ë¯¸ë§Œ
            print("ğŸ¯ ëª¨ë¸ ì˜ˆì¸¡ ë¶„ì‚° ìˆ˜ë ´ (< 5%)")
            return True
        
        return False
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° ë¦¬ìŠ¤í¬ ê´€ë¦¬

### 1. ê³¼ì í•© ë°©ì§€ ì „ëµ

```python
def prevent_overfitting():
    """ê³¼ì í•© ë°©ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸"""
    
    # âœ… BERT íŒŒì¸íŠœë‹ ì‹œ ì£¼ì˜ì‚¬í•­
    bert_precautions = {
        'learning_rate': 'encoderëŠ” 1e-5, classifierëŠ” 1e-3',
        'epochs': 'ìµœëŒ€ 5 ì—í­, ì¡°ê¸° ì¢…ë£Œ í•„ìˆ˜',
        'validation_monitoring': 'ê²€ì¦ ì†ì‹¤ ì¦ê°€ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨',
        'weight_decay': '0.01 ì ìš©'
    }
    
    # âœ… BiLSTM ë¶€ë¶„ ì¬í›ˆë ¨ ì‹œ ì£¼ì˜ì‚¬í•­
    bilstm_precautions = {
        'frozen_layers': 'ì„ë² ë”© ë ˆì´ì–´ ê³ ì • ìœ ì§€',
        'learning_rate': '1e-3ì—ì„œ ì‹œì‘, ì ì‘ì  ê°ì†Œ',
        'dropout': '0.2-0.3 ìœ ì§€',
        'gradient_clipping': '1.0 ì ìš©'
    }
```

### 2. ì•™ìƒë¸” ê· í˜• ìœ ì§€

```python
def maintain_ensemble_balance():
    """ì•™ìƒë¸” ê· í˜• ëª¨ë‹ˆí„°ë§"""
    
    # ê°œë³„ ëª¨ë¸ ê¸°ì—¬ë„ ì¶”ì 
    contribution_scores = {}
    
    for model_name in ['bert', 'bilstm', 'lstm', 'word_sentiment']:
        # ê°œë³„ ëª¨ë¸ ì •í™•ë„
        individual_acc = evaluate_individual_model(model_name)
        
        # ì•™ìƒë¸”ì—ì„œ ì œì™¸ ì‹œ ì„±ëŠ¥ í•˜ë½
        ensemble_without = evaluate_ensemble_without(model_name)
        contribution = ensemble_acc - ensemble_without
        
        contribution_scores[model_name] = contribution
    
    # ë¶ˆê· í˜• ê°ì§€
    max_contribution = max(contribution_scores.values())
    min_contribution = min(contribution_scores.values())
    
    if max_contribution / min_contribution > 3.0:
        print("âš ï¸  ì•™ìƒë¸” ë¶ˆê· í˜• ê°ì§€: ê°€ì¤‘ì¹˜ ì¬ì¡°ì • í•„ìš”")
        rebalance_ensemble_weights(contribution_scores)
```

### 3. ë°ì´í„° í¸í–¥ ë°©ì§€

```python
def detect_data_bias():
    """ë°ì´í„° í¸í–¥ ê°ì§€ ì‹œìŠ¤í…œ"""
    
    bias_checks = {
        'label_distribution': check_label_balance(),
        'domain_coverage': check_domain_diversity(),
        'refinement_patterns': check_refinement_bias(),
        'model_agreement': check_prediction_diversity()
    }
    
    # í¸í–¥ ê²½ê³ 
    for check_name, result in bias_checks.items():
        if result['bias_score'] > 0.7:
            print(f"âš ï¸  {check_name}ì—ì„œ í¸í–¥ ê°ì§€: {result['description']}")
```

---

## ğŸ¯ ì‹¤í–‰ ê³„íš ë° íƒ€ì„ë¼ì¸

### Phase 1: ì¦‰ì‹œ ì‹¤í–‰ (1ì¼)
```
ğŸ¯ ëª©í‘œ: Word Sentiment ì‚¬ì „ ì—…ë°ì´íŠ¸ ë° ê¸°ë°˜ ì‹œì„¤ êµ¬ì¶•

âœ… ì‘ì—… í•­ëª©:
- [ ] Word Sentiment ì‚¬ì „ í˜„ì¬ ìƒíƒœ ë¶„ì„
- [ ] ë°ì´í„° ì •ì œ ê²°ê³¼ ê¸°ë°˜ ì‚¬ì „ ì—…ë°ì´íŠ¸ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
- [ ] í†µí•© ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] MLflow ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ì •

ğŸ“ˆ ì˜ˆìƒ íš¨ê³¼: +2-4% ì„±ëŠ¥ í–¥ìƒ
```

### Phase 2: ë‹¨ê¸° ëª©í‘œ (3-5ì¼)
```
ğŸ¯ ëª©í‘œ: BiLSTM ë¶€ë¶„ ì¬í›ˆë ¨ + LSTM ì „ì´í•™ìŠµ

âœ… ì‘ì—… í•­ëª©:
- [ ] BiLSTM ë¶€ë¶„ ì¬í›ˆë ¨ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- [ ] LSTM ì „ì´í•™ìŠµ ìµœì í™”
- [ ] ì²« ë²ˆì§¸ ë°˜ë³µ ì •ì œ-ì¬í›ˆë ¨ ì‚¬ì´í´ ì‹¤í–‰
- [ ] ì´ˆê¸° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì„¤ì •

ğŸ“ˆ ì˜ˆìƒ íš¨ê³¼: ê¸°ì¤€ì„  ì„¤ì • + LSTM 3-5% í–¥ìƒ
```

### Phase 3: ì¤‘ê¸° ëª©í‘œ (1-2ì£¼)
```
ğŸ¯ ëª©í‘œ: BERT íŒŒì¸íŠœë‹ ìµœì í™”

âœ… ì‘ì—… í•­ëª©:
- [ ] BERT íŒŒì¸íŠœë‹ ì „ëµ ìµœì í™”
- [ ] ì ì§„ì  í•´ë™ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
- [ ] ì „ì²´ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì¬ì¡°ì •
- [ ] ì„±ëŠ¥ ìµœì í™” ë° íŠœë‹

ğŸ“ˆ ì˜ˆìƒ íš¨ê³¼: BERT 1-2% í–¥ìƒ, ì•™ìƒë¸” 5-10% í–¥ìƒ
```

### Phase 4: ì¥ê¸° ëª©í‘œ (3-4ì£¼)
```
ğŸ¯ ëª©í‘œ: ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸

âœ… ì‘ì—… í•­ëª©:
- [ ] ì™„ì „ ìë™í™”ëœ ë°˜ë³µ ì •ì œ-ì¬í›ˆë ¨ ì‹œìŠ¤í…œ
- [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- [ ] A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„

ğŸ“ˆ ì˜ˆìƒ íš¨ê³¼: 90-95% ì•™ìƒë¸” ì •í™•ë„ ë‹¬ì„±
```

---

## ğŸ† ê¸°ëŒ€ íš¨ê³¼ ë° KPI

### ì •ëŸ‰ì  ëª©í‘œ
- **ì•™ìƒë¸” ì •í™•ë„**: 90-95% ë‹¬ì„±
- **ê°œë³„ ëª¨ë¸ ì„±ëŠ¥**: ê°ê° 3-5% í–¥ìƒ
- **í›ˆë ¨ ì‹œê°„**: ì™„ì „ ì¬í›ˆë ¨ ëŒ€ë¹„ 70% ë‹¨ì¶•
- **ë°ì´í„° í’ˆì§ˆ**: ì •ì œìœ¨ 95% ì´ìƒ

### ì •ì„±ì  ëª©í‘œ
- **ì‹œìŠ¤í…œ ì•ˆì •ì„±**: ìë™í™”ëœ í’ˆì§ˆ ê´€ë¦¬
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ë„ë©”ì¸ ë°ì´í„° ì‰¬ìš´ ì ìš©
- **í•´ì„ ê°€ëŠ¥ì„±**: ëª¨ë¸ë³„ ê¸°ì—¬ë„ ëª…í™•í•œ ì¶”ì 
- **ìœ ì§€ ë³´ìˆ˜ì„±**: ëª¨ë“ˆí™”ëœ ì¬í›ˆë ¨ íŒŒì´í”„ë¼ì¸

---

## ğŸ“ ê²°ë¡ 

### í•µì‹¬ ì¸ì‚¬ì´íŠ¸

1. **ì „ëµì  ì°¨ë³„í™”ê°€ í•µì‹¬**: ëª¨ë¸ë³„ íŠ¹ì„±ì— ë§ëŠ” ì¬í›ˆë ¨ ë°©ë²•ë¡  ì ìš©
2. **ì ì§„ì  ê°œì„ ì˜ ìœ„ë ¥**: ë°ì´í„° ì •ì œì™€ ì—°ê³„í•œ ë°˜ë³µì  ìµœì í™”
3. **ë¹„ìš© íš¨ìœ¨ì„±**: ì™„ì „ ì¬í›ˆë ¨ ëŒ€ë¹„ 70% ì ì€ ë¹„ìš©ìœ¼ë¡œ 90% ì„±ëŠ¥ íš¨ê³¼
4. **ìë™í™”ì˜ ì¤‘ìš”ì„±**: ì§€ì† ê°€ëŠ¥í•œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸

### ìµœì¢… ê¶Œì¥ì‚¬í•­

**ì´ í”„ë¡œì íŠ¸ì˜ ì¬í›ˆë ¨ì€ "ì™„ì „ ì¬í›ˆë ¨"ì´ ì•„ë‹Œ "ì§€ëŠ¥ì  ë¶€ë¶„ ì¬í›ˆë ¨"ì…ë‹ˆë‹¤.**

ê° ëª¨ë¸ì˜ ê°•ì ì„ ì‚´ë¦¬ë©´ì„œë„ íš¨ìœ¨ì ìœ¼ë¡œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì°¨ë³„í™”ëœ ì ‘ê·¼ë²•ì„ í†µí•´, ìµœì†Œí•œì˜ ë¹„ìš©ìœ¼ë¡œ ìµœëŒ€í•œì˜ íš¨ê³¼ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

**ë³´ê³ ì„œ ì‘ì„±**: GitHub Copilot  
**ê²€í†  ë° ìŠ¹ì¸**: ê°œë°œíŒ€  
**ë‹¤ìŒ ì•¡ì…˜**: Phase 1 ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
